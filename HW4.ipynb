{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/3765 (0%)]\tLoss: 0.368959\n",
      "Train Epoch: 1 [640/3765 (17%)]\tLoss: 0.786761\n",
      "Train Epoch: 1 [1280/3765 (34%)]\tLoss: 0.343750\n",
      "Train Epoch: 1 [1920/3765 (51%)]\tLoss: 0.399792\n",
      "Train Epoch: 1 [2560/3765 (68%)]\tLoss: 0.575552\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from matplotlib.image import imread\n",
    "\n",
    "import torch#use: pip install torch, then restart the kernel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms#pip install torchvision\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Boats(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, transform=None, gt_json_path=''):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.gt_json_path = gt_json_path\n",
    "        self.labels = json.load(open(gt_json_path, 'r'))\n",
    "        self.image_list = sorted(os.listdir(root_dir))\n",
    "        self.image_ids = dict(enumerate(self.image_list, start=0))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.load_image(idx)\n",
    "        img_name = self.image_ids[idx]\n",
    "        label = self.labels[img_name]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        sample = (img, label)\n",
    "        return sample\n",
    "\n",
    "    def load_image(self, image_index):\n",
    "        image_name = self.image_ids[image_index]\n",
    "        path = os.path.join(self.root_dir, image_name)\n",
    "        img = imread(path)\n",
    "        return img\n",
    "\n",
    "\"\"\"\n",
    "Explanation my NN architecture:\n",
    "1. Added Batch Normalization (bn1, bn2) to stabilize and speed up training.\n",
    "2. Increased filters in conv1 (32) and conv2 (64) for better feature extraction.\n",
    "3. Used MaxPooling to downsample spatial dimensions and extract key features.\n",
    "4. Introduced Dropout (0.5) before the final layer to prevent overfitting.\n",
    "5. Adjusted fc1 input size to 64 * 27 * 48, matching the flattened tensor shape.\n",
    "6. Applied Sigmoid activation in fc2 for binary classification output.\n",
    "These changes enhance learning, reduce overfitting, and ensure compatibility with binary classification.\n",
    "\"\"\"\n",
    "class Net(nn.Module):#TODO 9) #creat your own NN architecture\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 27 * 48, 256)  #TODO Question 1) #This defines a fully connected layer with 64 * 27 * 48 input features (flattened image pixels) and 256 output feature (intermediate feature representation for further processing).\n",
    "        self.fc2 = nn.Linear(256, 1) # Input size: 256 (from fc1, Output size: 1 (for binary classification, producing a single probability value between 0 and 1).\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers with BatchNorm, ReLU, and Pooling\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        \n",
    "        # Flatten the tensor for fully connected layers\n",
    "        x = torch.flatten(x, start_dim=1)#TODO Question 2) # This flattens the input tensor (batch size, channels, height, width) into a 2D tensor (batch size, features), preparing it for the fully connected layer.\n",
    "        x = self.fc1(x)#TODO Question 3) # This applies the fully connected layer (fc1) to the input tensor\n",
    "\n",
    "        # Apply fully connected layers with Dropout\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        output = torch.sigmoid(self.fc2(x))  # Sigmoid activation for binary classification\n",
    "        return output\n",
    "\n",
    "\n",
    "def train(log_interval, model, device, train_loader, optimizer, criterion, epoch,dry_run):\n",
    "    \"\"\"\n",
    "    Train a network\n",
    "    You can find example code here: https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device).float()\n",
    "        optimizer.zero_grad()#TODO Question 4) # This clears the gradients from the previous training step to prevent accumulation during backpropagation.\n",
    "        output = model(data)\n",
    "        loss = criterion(output, torch.unsqueeze(target, 1))#TODO Question 5) # This computes the loss between the modelâ€™s output and the ground truth labels. The target is unsqueezed to match the dimensions of the output.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if dry_run:\n",
    "                break\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device).float()\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, torch.unsqueeze(target, 1)).item()  # sum up batch loss\n",
    "            pred = torch.round(output)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Training settings #you can mess around with change these values!\n",
    "    batch_size = 32\n",
    "    test_batch_size = 1000\n",
    "    epochs = 20\n",
    "    learning_rate = 0.001\n",
    "    no_cuda = True #If you using course cpu leave False, if you are using GPU set true\n",
    "    dry_run = False\n",
    "    seed = random.randint(1,1000)#random seed. Set to constant if you want to train on the same data\n",
    "    log_interval = 10#how many batches to wait before logging training status\n",
    "    save_model = False \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    #This is used if you want to run it as a script file.\n",
    "    parser = argparse.ArgumentParser(description='PyTorch Ship Detection')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=14, metavar='N',\n",
    "                        help='number of epochs to train (default: 14)')\n",
    "    parser.add_argument('--lr', type=float, default=0.1, metavar='LR',\n",
    "                        help='learning rate (default: 0.1)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "                        help='quickly check a single pass')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "    args = parser.parse_args()\n",
    "    \"\"\"\n",
    "    #torch.manual_seed(args.seed)\n",
    "    torch.manual_seed(seed)\n",
    "    #use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    use_cuda = no_cuda\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    #train_kwargs = {'batch_size': args.batch_size}\n",
    "    #val_kwargs = {'batch_size': args.test_batch_size}\n",
    "    train_kwargs = {'batch_size': batch_size}\n",
    "    val_kwargs = {'batch_size': test_batch_size}\n",
    "    if use_cuda:\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        val_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    # Create transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        # This normalization is used on the test server\n",
    "        transforms.Normalize([0.2404, 0.2967, 0.3563], [0.0547, 0.0527, 0.0477])\n",
    "        ])\n",
    "\n",
    "    # Create train and test set\n",
    "    path_to_dataset = \"/courses/CS5330.202510/data/Boat-MNIST\"#gobal path to the data on Discovery \n",
    "    train_set = Boats(root_dir=path_to_dataset + \"/train\", transform=transform,\n",
    "                      gt_json_path=path_to_dataset + \"/boat_mnist_labels_trainval.json\")\n",
    "    val_set = Boats(root_dir=path_to_dataset + \"/val\", transform=transform,\n",
    "                    gt_json_path=path_to_dataset +\"/boat_mnist_labels_trainval.json\")\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, **train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(val_set, **val_kwargs)\n",
    "\n",
    "    # Create network, optimizer and loss\n",
    "    model = Net().to(device)#TODO Question 6) # This initializes the Net model and moves it to the specified device (CPU or GPU).\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)#TODO Question 7) # This initializes the stochastic gradient descent (SGD) optimizer, specifying which parameters to update and the learning rate.\n",
    "    criterion = nn.MSELoss()#TODO Question 8) # This sets the mean squared error (MSE) loss function, which is used to evaluate the difference between the predicted and true values.\n",
    "\n",
    "    # Train and validate\n",
    "    best_acc = 0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(log_interval, model, device, train_loader, optimizer, criterion, epoch, dry_run)\n",
    "        acc = test(model, device, test_loader, criterion)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    print(f\"Best accuracy (val): {best_acc}\")\n",
    "\n",
    "    #if args.save_model:\n",
    "    #    torch.save(model.state_dict(), \"model.pth\")\n",
    "    if save_model:\n",
    "        torch.save(model.state_dict(), \"model.pth\")\n",
    "    \n",
    "    # --- Do not touch -----\n",
    "    # Save model as onnx file\n",
    "    dummy_input = torch.randn(1, 3, 108, 192, device=device)\n",
    "    input_names = [\"img_1\"]\n",
    "    output_names = [\"output1\"]\n",
    "    torch.onnx.export(model, dummy_input, \"ship_example.onnx\", input_names=input_names, output_names=output_names)\n",
    "    # ----------------------\n",
    "\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
